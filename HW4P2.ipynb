{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobrast/11785-HW4P2/blob/main/HW4P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8a_cZqr-UpV"
      },
      "source": [
        "# HW4P2: Attention-based Speech Recognition\n",
        "\n",
        "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with attention. <br> <br>\n",
        "\n",
        "HW Writeup: https://piazza.com/class_profile/get_resource/l37uyxe87cq5xn/lam1lcjjj0314e <br>\n",
        "Kaggle competition link: https://www.kaggle.com/competitions/11-785-f22-hw4p2/ <br>\n",
        "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
        "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlev_Tvq_bRz"
      },
      "source": [
        "# Initial Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "0pueIzbxUwyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcef0155-1424-4465-bce7-440de28ad5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 11 12:59:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0    31W /  70W |   2332MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "tepaid_TDwWt"
      },
      "outputs": [],
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr4xGzRU-KZz"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "ZectxKF3XEVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a21427-db21-4f81-d66c-e00811f2511d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from torchsummaryX import summary\n",
        "import wandb\n",
        "from glob import glob\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "WFCoftyeCpG6"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "K5HLepZpA5WS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3398e51-424f-4f39-ec5b-524910491995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# TODO: Import drive if you are a colab user\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALQCI0EDCwh"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "AqDuibMCP345"
      },
      "outputs": [],
      "source": [
        "# Global config dict. Feel free to add or change if you want.\n",
        "config = {\n",
        "    'batch_size': 96,\n",
        "    'epochs': 30,\n",
        "    'lr': 1e-3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7J4sY1OW9Pr"
      },
      "source": [
        "# Toy Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsQB-pvRLLs"
      },
      "source": [
        "The toy dataset is very essential for you in this HW. The model which you will be building is complicated and you first need to make sure that it runs on the toy dataset. <br>\n",
        "In other words, you need convergence - the attention diagonal. Take a look at the write-up for this. <br>\n",
        "We have given you the following code to download the toy data and load it. You can use it the way it is. But be careful, the transcripts are different from the original data from kaggle. The toy dataset has phonemes but the actual data has characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "Des4AMIaW4E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48d6407-7b87-4006-f38c-bada20fa35f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f0176_mfccs_train.n 100%[===================>] 279.30M  14.8MB/s    in 22s     \n",
            "f0176_mfccs_dev.npy 100%[===================>]  27.93M  14.6MB/s    in 1.9s    \n",
            "f0176_hw3p2_train.n 100%[===================>]   7.02M  6.13MB/s    in 1.1s    \n",
            "f0176_hw3p2_dev.npy 100%[===================>] 718.88K  1008KB/s    in 0.7s    \n"
          ]
        }
      ],
      "source": [
        "!wget -q https://cmu.box.com/shared/static/wok08c2z2dp4clufhy79c5ee6jx3pyj9 --content-disposition --show-progress\n",
        "!wget -q https://cmu.box.com/shared/static/zctr6mvh7npfn01forli8n45duhp2g85 --content-disposition --show-progress\n",
        "!wget -q https://cmu.box.com/shared/static/m2oaek69145ljeu6srtbbb7k0ip6yfup --content-disposition --show-progress\n",
        "!wget -q https://cmu.box.com/shared/static/owrjy0tqra3v7zq2ru7mocy2djskydy9 --content-disposition --show-progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "2fLveDeiXCsb"
      },
      "outputs": [],
      "source": [
        "# Load the toy dataset\n",
        "X_train = np.load(\"f0176_mfccs_train.npy\")\n",
        "X_valid = np.load(\"f0176_mfccs_dev.npy\")\n",
        "Y_train = np.load(\"f0176_hw3p2_train.npy\")\n",
        "Y_valid = np.load(\"f0176_hw3p2_dev.npy\")\n",
        "\n",
        "# This is how you actually need to find out the different trancripts in a dataset. \n",
        "# Can you think whats going on here? Why are we using a np.unique?\n",
        "VOCAB_MAP           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid))))) \n",
        "VOCAB_MAP[\"[PAD]\"]  = len(VOCAB_MAP)\n",
        "VOCAB               = list(VOCAB_MAP.keys())\n",
        "\n",
        "SOS_TOKEN = VOCAB_MAP[\"[SOS]\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"[EOS]\"]\n",
        "PAD_TOKEN = VOCAB_MAP[\"[PAD]\"]\n",
        "\n",
        "Y_train = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_train]\n",
        "Y_valid = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_valid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "iNjjsFqpbocR"
      },
      "outputs": [],
      "source": [
        "# Dataset class for the Toy dataset\n",
        "class ToyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition):\n",
        "\n",
        "        if partition == \"train\":\n",
        "            self.mfccs = X_train[:, :, :15]\n",
        "            self.transcripts = Y_train\n",
        "\n",
        "        elif partition == \"valid\":\n",
        "            self.mfccs = X_valid[:, :, :15]\n",
        "            self.transcripts = Y_valid\n",
        "\n",
        "        assert len(self.mfccs) == len(self.transcripts)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        x = torch.tensor(self.mfccs[i])\n",
        "        y = torch.tensor(self.transcripts[i])\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        x_batch, y_batch = list(zip(*batch))\n",
        "\n",
        "        x_lens      = [x.shape[0] for x in x_batch] \n",
        "        y_lens      = [y.shape[0] for y in y_batch] \n",
        "\n",
        "        x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN)\n",
        "        y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN) \n",
        "        \n",
        "        return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWRjucnUdbQ1"
      },
      "source": [
        "# Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "fStTuAQ6XAuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf4c736-2017-4d73-d1ae-86b4d3877a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n"
          ]
        }
      ],
      "source": [
        "# # TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n",
        "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "# !mkdir /root/.kaggle/\n",
        "\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "#     f.write('{\"username\":\"xuezhe\",\"key\":\"c860ab9f96d5be4964527a0c38aa2422\"}') # Put your kaggle username & key here\n",
        "\n",
        "# !chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "nR74ooCSa664",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022cc0cb-c5a5-4fad-eca5-59ddd202c7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-f22-hw4p2.zip to /content\n",
            "100% 2.08G/2.09G [00:53<00:00, 40.6MB/s]\n",
            "100% 2.09G/2.09G [00:53<00:00, 41.6MB/s]\n",
            "mkdir: cannot create directory ‘/content/data’: File exists\n"
          ]
        }
      ],
      "source": [
        "# # Download the data\n",
        "# !kaggle competitions download -c 11-785-f22-hw4p2\n",
        "# !mkdir '/content/data'\n",
        "\n",
        "# !unzip -qo '11-785-f22-hw4p2.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ioyn6ldQB9"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YHix8UvpBWh7"
      },
      "outputs": [],
      "source": [
        "# # These are the various characters in the transcripts of the datasetW\n",
        "# VOCAB = ['<sos>',   \n",
        "#          'A',   'B',    'C',    'D',    \n",
        "#          'E',   'F',    'G',    'H',    \n",
        "#          'I',   'J',    'K',    'L',       \n",
        "#          'M',   'N',    'O',    'P',    \n",
        "#          'Q',   'R',    'S',    'T', \n",
        "#          'U',   'V',    'W',    'X', \n",
        "#          'Y',   'Z',    \"'\",    ' ', \n",
        "#          '<eos>']\n",
        "\n",
        "# VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "# SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "# EOS_TOKEN = VOCAB_MAP[\"<eos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "# # TODO: Create a dataset class which is exactly the same as HW3P2. You are free to reuse it. \n",
        "# # The only change is that the transcript mapping is different for this HW.\n",
        "# # Note: We also want to retain SOS and EOS tokens in the transcript this time.\n",
        "\n",
        "# class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "#     # For this homework, we give you full flexibility to design your data set class.\n",
        "#     # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "#     #TODO\n",
        "#     def __init__(self, data_path): \n",
        "#         '''\n",
        "#         Initializes the dataset.\n",
        "\n",
        "#         INPUTS: What inputs do you need here?\n",
        "#         '''\n",
        "\n",
        "#         # Load the directory and all files in them\n",
        "\n",
        "#         self.mfcc_dir = os.path.join(data_path, \"mfcc\")\n",
        "#         self.transcript_dir = os.path.join(data_path, \"transcript\", \"raw\")\n",
        "\n",
        "#         self.mfcc_files = os.listdir(self.mfcc_dir)[:100]\n",
        "#         self.transcript_files = os.listdir(self.transcript_dir)[:100]\n",
        "\n",
        "#         #self.PHONEMES = PHONEMES\n",
        "#         self.PHONEMES = VOCAB #????\n",
        "\n",
        "#         #TODO\n",
        "#         # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "#         self.length = len(self.mfcc_files)\n",
        "        \n",
        "#         #TODO\n",
        "#         # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "#         # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "#         mfccs, transcripts = [], []\n",
        "\n",
        "#         for i in range(self.length):\n",
        "#             mfcc = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))\n",
        "#             transcript = np.load(os.path.join(self.transcript_dir, self.transcript_files[i]))\n",
        "\n",
        "#             mfccs.append(mfcc)\n",
        "#             transcripts.append(transcript)\n",
        "\n",
        "#         for i in range(self.length):\n",
        "#             for j in range(len(self.PHONEMES)):\n",
        "#                 transcripts[i] = np.where(transcripts[i] == self.PHONEMES[j], j, transcripts[i])\n",
        "\n",
        "#             transcripts[i] = transcripts[i].astype(int)\n",
        "\n",
        "#         #TODO\n",
        "#         # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "#         # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "#         '''\n",
        "#         You may decide to do this in __getitem__ if you wish.\n",
        "#         However, doing this here will make the __init__ function take the load of\n",
        "#         loading the data, and shift it away from training.\n",
        "#         '''\n",
        "       \n",
        "#         self.transcripts = transcripts\n",
        "#         self.mfccs = mfccs\n",
        "\n",
        "#     def __len__(self):\n",
        "        \n",
        "#         '''\n",
        "#         TODO: What do we return here?\n",
        "#         '''\n",
        "#         return self.length\n",
        "\n",
        "#     def __getitem__(self, ind):\n",
        "#         '''\n",
        "#         TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "#         If you didn't do the loading and processing of the data in __init__,\n",
        "#         do that here.\n",
        "\n",
        "#         Once done, return a tuple of features and labels.\n",
        "#         '''\n",
        "#         mfcc = torch.tensor(self.mfccs[ind])\n",
        "#         transcript = torch.tensor(self.transcripts[ind])\n",
        "#         return mfcc, transcript\n",
        "\n",
        "\n",
        "#     def collate_fn(self,batch):\n",
        "#         '''\n",
        "#         TODO:\n",
        "#         1.  Extract the features and labels from 'batch'\n",
        "#         2.  We will additionally need to pad both features and labels,\n",
        "#             look at pytorch's docs for pad_sequence\n",
        "#         3.  This is a good place to perform transforms, if you so wish. \n",
        "#             Performing them on batches will speed the process up a bit.\n",
        "#         4.  Return batch of features, labels, lenghts of features, \n",
        "#             and lengths of labels.\n",
        "#         '''\n",
        "#         # batch of input mfcc coefficients\n",
        "#         batch_mfcc = [d[0] for d in batch]\n",
        "#         # batch of output phonemes\n",
        "#         batch_transcript = [d[1] for d in batch]\n",
        "\n",
        "#         # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "#         # Also be sure to check the input format (batch_first)\n",
        "\n",
        "#         batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
        "#         lengths_mfcc = [len(d) for d in batch_mfcc]\n",
        "\n",
        "#         batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "#         lengths_transcript = [len(d) for d in batch_transcript]\n",
        "\n",
        "#         # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "#         # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "#         #                  -> Would we apply transformation on the validation set as well?\n",
        "#         #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "#         # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "#         return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zannKblcPORG"
      },
      "outputs": [],
      "source": [
        "# TODO: Similarly, create a test dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQenneVsDLnX"
      },
      "source": [
        "# Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "oi2FS_hkDQZB"
      },
      "outputs": [],
      "source": [
        "# # TODO: Create the datasets and dataloaders\n",
        "# # All these things are similar to HW3P2\n",
        "# # You can reuse the same code\n",
        "# train_data = AudioDataset(\"/content/data/hw4p2/train-clean-100\") #TODO\n",
        "# val_data = AudioDataset(\"/content/data/hw4p2/dev-clean\")\n",
        "# #test_data = AudioDatasetTest() #TODO\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=config['batch_size'], collate_fn=train_data.collate_fn)\n",
        "# val_loader = DataLoader(val_data, batch_size=config['batch_size'], collate_fn=val_data.collate_fn)\n",
        "# #test_loader = #TODO\n",
        "\n",
        "# # The sanity check for shapes also are similar\n",
        "# # Please remember that the only change in the dataset for this HW is the transcripts\n",
        "# # So you are expected to get similar shapes like HW3P2 (Pad, pack and Oh my!)\n",
        "# print(\"Batch size: \", config['batch_size'])\n",
        "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "# #print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ToyDataset(partition=\"train\")\n",
        "val_data = ToyDataset(partition=\"valid\")\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=config['batch_size'], collate_fn=train_data.collate_fn)\n",
        "val_loader = DataLoader(val_data, batch_size=config['batch_size'], collate_fn=val_data.collate_fn)\n",
        "\n",
        "# The sanity check for shapes also are similar\n",
        "# Please remember that the only change in the dataset for this HW is the transcripts\n",
        "# So you are expected to get similar shapes like HW3P2 (Pad, pack and Oh my!)\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "#print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZKBpCuH7Yrh",
        "outputId": "67fb33e1-c186-4042-9ce4-f24d212e026f"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  96\n",
            "Train dataset samples = 16000, batches = 167\n",
            "Val dataset samples = 1600, batches = 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--VjKlEhwi8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uql9E6cqROvJ"
      },
      "source": [
        "In this section you will be building the LAS model from scratch. Before starting to code, please read the writeup, paper and understand the following parts completely.<br>\n",
        "- Pyramidal Bi-LSTM \n",
        "- Listener\n",
        "- Attention\n",
        "- Speller\n",
        "\n",
        "After getting a good grasp of the workings of these modules, start coding. Follow the TODOs carefully. We will also be adding some extra features to the attention mechanism like keys and values which are not originally present in LAS. So we will be creating a hybrid network based on LAS and Attention is All You Need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCbwz0LZMWwe"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoI0zEoIMX5I"
      },
      "source": [
        "### Pyramidal Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "-F9zAQR95P55"
      },
      "outputs": [],
      "source": [
        "# class pBLSTM(torch.nn.Module):\n",
        "\n",
        "#     '''\n",
        "#     Pyramidal BiLSTM\n",
        "#     Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "#     At each step,\n",
        "#     1. Pad your input if it is packed (Unpack it)\n",
        "#     2. Reduce the input length dimension by concatenating feature dimension\n",
        "#         (Tip: Write down the shapes and understand)\n",
        "#         (i) How should  you deal with odd/even length input? \n",
        "#         (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "#     3. Pack your input\n",
        "#     4. Pass it into LSTM layer\n",
        "\n",
        "#     To make our implementation modular, we pass 1 layer at a time.\n",
        "#     '''\n",
        "    \n",
        "#     def __init__(self, input_size, hidden_size):\n",
        "#         super(pBLSTM, self).__init__()\n",
        "#         self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=True)\n",
        "\n",
        "#     def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "#         # TODO: Pad Packed Sequence\n",
        "#         #pad, lengths_pad = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)\n",
        "        \n",
        "#         # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "#         #x, x_lens = self.trunc_reshape(pad, lengths_pad)\n",
        "#         x, _ = self.trunc_reshape(x_packed, 0)\n",
        "\n",
        "#         # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "#         # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "#         #x = nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "#         # TODO: Pass the sequence through bLSTM\n",
        "#         # Do we need hidden layer?\n",
        "#         #y, (h, _) = self.blstm(xn\n",
        "        \n",
        "#         y, _ = self.lstm(x)\n",
        "\n",
        "#         return y\n",
        "\n",
        "#     def trunc_reshape(self, x, x_lens): \n",
        "#         # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "#         # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "#         # TODO: Reduce lengths by the same downsampling factor\n",
        "\n",
        "#         # Throw away last timestep in case of odd sized input\n",
        "#         if x.shape[1] % 2:\n",
        "#             x = x[:, :-1, :]\n",
        "\n",
        "#         x = x.reshape(x.shape[0], int(x.shape[1] / 2), -1)\n",
        "\n",
        "\n",
        "#         return x, x_lens // 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rchbyjlMeB2"
      },
      "source": [
        "### Listener"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "912b3sVoHr1e"
      },
      "outputs": [],
      "source": [
        "# class Listener(torch.nn.Module):\n",
        "#     '''\n",
        "#     The Encoder takes utterances as inputs and returns latent feature representations\n",
        "#     '''\n",
        "#     def __init__(self, input_size, encoder_hidden_size):\n",
        "#         super(Listener, self).__init__()\n",
        "\n",
        "#         # The first LSTM at the very bottom\n",
        "#         self.base_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=encoder_hidden_size,  bidirectional=True)#TODO Double check that I am using encoder_hidden_size correctly\n",
        "\n",
        "#         self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
        "#             # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
        "#             # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "#             # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "#             pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "#             pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "#             pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "#         )\n",
        "         \n",
        "#     def forward(self, x, x_lens):\n",
        "#         # Where are x and x_lens coming from? The dataloader\n",
        "        \n",
        "#         # TODO: Pack Padded Sequence\n",
        "#         # TODO: Pass it through the first LSTM layer (no truncation)\n",
        "#         # TODO: Pad Packed Sequence\n",
        "#         # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "\n",
        "#         # Remember the number of output(s) each function returns\n",
        "#         #x = nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "#         x, _ = self.base_lstm(x) # Question: What is the hidden size?\n",
        "\n",
        "#         x = self.pBLSTMs(x)\n",
        "        \n",
        "\n",
        "#         #encoder_outputs, encoder_lens = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "#         #return encoder_outputs, encoder_lens\n",
        "\n",
        "#         return x, x_lens // 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "id": "0EgRqsKDI7dD"
      },
      "outputs": [],
      "source": [
        "# # Question: do we need to treat the hidden size as a hyperparameter?\n",
        "# # Question: what is the input size? 1 for audio data?\n",
        "\n",
        "# encoder = Listener(15, 20) # TODO: Initialize Listener\n",
        "# print(encoder)\n",
        "\n",
        "# # Question: what is this example_batch?\n",
        "\n",
        "# #summary(encoder, example_batch[0].to(DEVICE), example_batch[3])\n",
        "# del encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "\n",
        "        # self.embedding = nn.Sequential(\n",
        "        #     nn.Conv1d(\n",
        "        #         in_channels = input_size,\n",
        "        #         out_channels = input_size,\n",
        "        #         kernel_size = 5,\n",
        "        #         stride = 2,\n",
        "        #         padding = 2 # ask about stride / padding ratio, and what stride is effectively doing here\n",
        "        #     ),\n",
        "        #     nn.BatchNorm1d(input_size),\n",
        "        #     nn.GELU(),\n",
        "        #     nn.Dropout(0.1),\n",
        "\n",
        "        # )\n",
        "        # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "        self.blstm = nn.LSTM( \n",
        "                            batch_first = True, \n",
        "                            input_size = input_size, \n",
        "                            hidden_size = hidden_size, \n",
        "                            num_layers = 1,\n",
        "                            # dropout = 0.15,\n",
        "                            bidirectional=True) \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "        # TODO: Pad Packed Sequence\n",
        "        pad, lengths_pad = pad_packed_sequence(x_packed, batch_first=True)\n",
        "\n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "\n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "        x, x_lens = self.trunc_reshape(pad, lengths_pad)\n",
        "\n",
        "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "\n",
        "        packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "\n",
        "        # TODO: Pass the sequence through bLSTM\n",
        "        output, ___ = self.blstm(packed)\n",
        "        return output\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        if (x.shape[1] % 2 == 1):\n",
        "          x = x[:, 0:(x.shape[1]-1), :]\n",
        "        x_reshaped = torch.reshape(x, shape = (x.shape[0], x.shape[1] // 2, x.shape[2] * 2))\n",
        "        x_lens_reshaped = x_lens // 2\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        return x_reshaped, x_lens_reshaped\n",
        "\n",
        "class Listener(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Listener, self).__init__()\n",
        "\n",
        "        # The first LSTM at the very bottom\n",
        "\n",
        "        self.conv1d = nn.Conv1d(input_size, input_size, kernel_size=1)\n",
        "        self.blstm = torch.nn.LSTM(batch_first = True, \n",
        "                            input_size = input_size, \n",
        "                            hidden_size = encoder_hidden_size, \n",
        "                            num_layers = 1,\n",
        "                            bidirectional = True,\n",
        "        )#TODO: Fill this up\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
        "            # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
        "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "            # ...\n",
        "            # ...\n",
        "      \n",
        "            pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "            # nn.Dropout(p=0.2),\n",
        "            pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "            # nn.Dropout(p=0.2),\n",
        "            pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "            # nn.Dropout(p=0.2),\n",
        "        )\n",
        "\n",
        "            \n",
        "         \n",
        "    def forward(self, x, x_lens):\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        \n",
        "\n",
        "        # # Batch_size, Seq_len, Features\n",
        "        # x = x.permute((0,2,1))\n",
        "        # # Batch_size, Features, Seq_len\n",
        "        # x = self.embedding(x)\n",
        "        # # Batch_size, New_Features, Seq_len//(product of strides)\n",
        "        # x = x.permute((0,2,1))\n",
        "        # # Batch_size, Seq_len//(product of strides), New_features\n",
        "\n",
        "        # # Reduce length by a factor of product of strides\n",
        "        # lx = torch.clamp(lx, max=x.shape[1])\n",
        "      \n",
        "        # TODO: Pack Padded Sequence\n",
        "        packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # TODO: Pass it through the first LSTM layer (no truncation)\n",
        "        output, ___ = self.blstm(packed)\n",
        "\n",
        "        # TODO: Pad Packed Sequence\n",
        "\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        \n",
        "        encoder_outputs = self.pBLSTMs(output)\n",
        "\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
        "\n",
        "        # return self.logSoftmax(classification), lengths_pad\n",
        "\n",
        "        # Remember the number of output(s) each function returns\n",
        "\n",
        "\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ],
      "metadata": {
        "id": "AZRlgEqzZ6eT"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJCpBcEmMVcZ"
      },
      "source": [
        "## Attention (Attend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6k9R7jKMRcZ"
      },
      "source": [
        "### Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query) \n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention): \n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n",
        "    Here are different ways to compute attention and context:\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n",
        "    attention           = softmax(masked_raw_weights)\n",
        "    context             = bmm(attention, value)\n",
        "    \n",
        "    At the end, you can pass context through a linear layer too.\n",
        "\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, encoder_hidden_size, decoder_output_size, projection_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.decoder_output_size = decoder_output_size\n",
        "        self.projection_size = projection_size\n",
        "\n",
        "        # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n",
        "        self.key_projection     = nn.Linear(encoder_hidden_size * 2, projection_size)\n",
        "        # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n",
        "        self.value_projection   = nn.Linear(encoder_hidden_size * 2, projection_size)\n",
        "        # TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n",
        "        self.query_projection   = nn.Linear(decoder_output_size, projection_size)\n",
        "        # Optional : Define an nn.Linear layer which projects the context vector\n",
        "\n",
        "        # TODO: Define a softmax layer. Think about the dimension which you need to apply \n",
        "        self.softmax            = nn.Softmax(dim=1) \n",
        "        # Tip: What is the shape of energy? And what are those?\n",
        "\n",
        "    # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n",
        "    # This function is used to calculate them and set them to self\n",
        "    def set_key_value_mask(self, encoder_outputs, encoder_lens):  \n",
        "        _, encoder_max_seq_len, _ = encoder_outputs.shape\n",
        "\n",
        "        self.key      = self.key_projection(encoder_outputs)# TODO: Project encoder_outputs using key_projection to get keys\n",
        "        self.value    = self.value_projection(encoder_outputs)# TODO: Project encoder_outputs using value_projection to get values\n",
        "\n",
        "        # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n",
        "        # The raw_weights are of shape (batch_size, timesteps)\n",
        "\n",
        "        # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n",
        "        # The mask is False for all indicies before padding begins, True for all indices after.\n",
        "        # TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n",
        "\n",
        "        # Jacob's Note: this code is largely from SO https://stackoverflow.com/questions/53403306/how-to-batch-convert-sentence-lengths-to-masks-in-pytorch\n",
        "        # Jacob's Note: This code needs to be tested...\n",
        "        a    =  torch.arange(encoder_max_seq_len).to(DEVICE) < encoder_lens.unsqueeze(dim=1).to(DEVICE)\n",
        "        self.padding_mask = a\n",
        "        # (Hint: Broadcasting gives you a one liner) # Broadcasting??  In my notebook????\n",
        "        \n",
        "    def forward(self, decoder_output_embedding):\n",
        "        # key   : (batch_size, timesteps, projection_size)\n",
        "        # value : (batch_size, timesteps, projection_size)\n",
        "        # query : (batch_size, projection_size)\n",
        "\n",
        "        # TODO: Project the query using query_projection\n",
        "        self.query         = self.query_projection(decoder_output_embedding).unsqueeze(dim=2)\n",
        "        # Hint: Take a look at torch.bmm for the products below \n",
        "\n",
        "        # TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n",
        "\n",
        "        raw_weights        = torch.bmm(self.key, self.query) / math.sqrt(self.key.size(-1))\n",
        "        raw_weights = raw_weights.squeeze(dim=-1)\n",
        "        # TODO: Mask the raw_weights with self.padding_mask. \n",
        "\n",
        "        masked_raw_weights = raw_weights.masked_fill(self.padding_mask==0, -1e9)\n",
        "        # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n",
        "\n",
        "        # TODO: Calculate the attention weights, which is the softmax of raw_weights\n",
        "        attention_weights  = self.softmax(masked_raw_weights).unsqueeze(dim=1)\n",
        "        # TODO: Calculate the context - it is a product between attention_weights and value\n",
        "        context            = torch.bmm(attention_weights, self.value).squeeze(dim=1)\n",
        "\n",
        "        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n",
        "\n",
        "        # Return the context, attention_weights\n",
        "        return context, attention_weights "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pandas.core.arrays import boolean\n",
        "# def plot_attention(attention): \n",
        "#     # Function for plotting attention\n",
        "#     # You need to get a diagonal plot\n",
        "#     plt.clf()\n",
        "#     sns.heatmap(attention, cmap='GnBu')\n",
        "#     plt.show()\n",
        "\n",
        "# class Attention(torch.nn.Module):\n",
        "#     '''\n",
        "#     Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n",
        "#     Here are different ways to compute attention and context:\n",
        "\n",
        "#     After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "#     masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n",
        "#     attention           = softmax(masked_raw_weights)\n",
        "#     context             = bmm(attention, value)\n",
        "    \n",
        "#     At the end, you can pass context through a linear layer too.\n",
        "\n",
        "#     '''\n",
        "    \n",
        "#     def __init__(self, encoder_hidden_size, decoder_output_size, projection_size):\n",
        "#         super(Attention, self).__init__()\n",
        "\n",
        "#         self.key_projection     = nn.Linear(encoder_hidden_size * 2, projection_size) # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n",
        "#         self.value_projection   = nn.Linear(encoder_hidden_size * 2, projection_size) # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n",
        "#         self.query_projection   = nn.Linear(decoder_output_size, projection_size) # TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n",
        "#         # Optional : Define an nn.Linear layer which projects the context vector\n",
        "\n",
        "#         self.softmax            = nn.Softmax(dim = 1)\n",
        "#         self.projection_size = projection_size\n",
        "#         # TODO: Define a softmax layer. Think about the dimension which you need to apply \n",
        "#         # Tip: What is the shape of energy? And what are those?\n",
        "        \n",
        "      \n",
        "#         # query_T * keys ... but how does this change shape?\n",
        "\n",
        "\n",
        "#     # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n",
        "#     # This function is used to calculate them and set them to self\n",
        "#     def set_key_value_mask(self, encoder_outputs, encoder_lens):\n",
        "#         # print(\"Encoder Shape:\", encoder_lens.shape)\n",
        "    \n",
        "#         _, encoder_max_seq_len, _ = encoder_outputs.shape\n",
        "\n",
        "#         self.key      = self.key_projection(encoder_outputs) # TODO: Project encoder_outputs using key_projection to get keys\n",
        "#         self.value    = self.key_projection(encoder_outputs) # TODO: Project encoder_outputs using value_projection to get values\n",
        "\n",
        "#         # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n",
        "#         # The raw_weights are of shape (batch_size, timesteps)\n",
        "\n",
        "#         # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n",
        "#         # The mask is False for all indicies before padding begins, True for all indices after.\n",
        "\n",
        "#         mask_tmp = []\n",
        "#         for b in range(self.key.shape[0]): # batch_size\n",
        "#           mask_batch = []\n",
        "#           for t in range(self.key.shape[1]): # for t in timesteps\n",
        "#               if (t < encoder_lens[b]):\n",
        "#                 mask_apply = False\n",
        "#               else: mask_apply = True\n",
        "#               mask_batch.append(mask_apply)\n",
        "#           mask_tmp.append(mask_batch)\n",
        "        \n",
        "#         self.padding_mask     =  torch.tensor(mask_tmp).bool().to(DEVICE) # TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n",
        "\n",
        "#     def forward(self, decoder_output_embedding):\n",
        "#         # key   : (batch_size, timesteps, projection_size)\n",
        "#         # value : (batch_size, timesteps, projection_size)\n",
        "#         # query : (batch_size, projection_size)\n",
        "#         # batch_size, timesteps, projection_size = self.key.shape\n",
        "\n",
        "#         self.query = self.query_projection(decoder_output_embedding) # TODO: Project the query using query_projection\n",
        "\n",
        "\n",
        "#         # Hint: Take a look at torch.bmm for the products below \n",
        "#         query = torch.unsqueeze(self.query, dim = 2)\n",
        "\n",
        "#         raw_weights = torch.bmm(self.key, query)\n",
        "\n",
        "#         raw_weights = torch.squeeze(raw_weights, dim = -1)\n",
        "\n",
        "        \n",
        "#         # TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n",
        "#         raw_weights.masked_fill_(self.padding_mask, -float(\"inf\")) # TODO: Mask the raw_weights with self.padding_mask. \n",
        "#         # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n",
        "\n",
        "#         attention_weights  = self.softmax(raw_weights)# TODO: Calculate the attention weights, which is the softmax of raw_weights\n",
        "\n",
        "\n",
        "#         # attention shape: (batch_size, timesteps)\n",
        "#         attention_weights = torch.unsqueeze(attention_weights, dim = 1)\n",
        "\n",
        "#         context            = torch.bmm(attention_weights, self.value) # TODO: Calculate the context - it is a product between attention_weights and value\n",
        "#         attention_weights = torch.squeeze(attention_weights, dim = 1)\n",
        "#         context = torch.squeeze(context, dim = 1)\n",
        "#         # assert(context.shape == (batch_size, projection_size))\n",
        "#         # assert(attention_weights.shape == (batch_size, timesteps))\n",
        "#         # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n",
        "\n",
        "#         return context, attention_weights # Return the context, attention_weights"
      ],
      "metadata": {
        "id": "tGo4WWtXYsLi"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78XOdWExMSi-"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuUQTy2NMlbT"
      },
      "source": [
        "### Speller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "y7-R6BTuT8dm"
      },
      "outputs": [],
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size         = vocab_size\n",
        "\n",
        "        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n",
        "        self.embedding          = nn.Embedding(vocab_size, embed_size, padding_idx=EOS_TOKEN)\n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "                                # Create Two LSTM Cells as per LAS Architecture\n",
        "                                # What should the input_size of the first LSTM Cell? \n",
        "                                # Hint: It takes in a combination of the character embedding and context from attention\n",
        "                                    nn.LSTMCell(embed_size + attention_module.projection_size, decoder_hidden_size),\n",
        "                                    nn.LSTMCell(decoder_hidden_size, decoder_output_size)\n",
        "                                )\n",
        "    \n",
        "                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n",
        "                                # Think why we need this in terms of the query\n",
        "\n",
        "        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n",
        "        self.char_prob          = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "        self.char_prob.weight   = self.embedding.weight # Weight tying\n",
        "\n",
        "        self.attention          = attention_module\n",
        "\n",
        "    \n",
        "    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1): \n",
        "\n",
        "        '''\n",
        "        Args: \n",
        "            embedding: Attention embeddings \n",
        "            hidden_list: List of Hidden States for the LSTM Cells\n",
        "        ''' \n",
        "\n",
        "        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape\n",
        "\n",
        "        if self.training:\n",
        "            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n",
        "            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n",
        "        else:\n",
        "            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n",
        "        \n",
        "\n",
        "        # INITS\n",
        "        predictions     = []\n",
        "\n",
        "        # Initialize the first character input to your decoder, SOS\n",
        "        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n",
        "\n",
        "        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n",
        "        hidden_states   = [None]*len(self.lstm_cells) \n",
        "\n",
        "        attention_plot          = []\n",
        "        \n",
        "        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n",
        "\n",
        "        # Set Attention Key, Value, Padding Mask just once\n",
        "        if self.attention != None:\n",
        "            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n",
        "\n",
        "        # TODO: Initialize context (You have a few choices, refer to the writeup )\n",
        "        #context, _                = self.attention(torch.zeros(batch_size, self.attention.decoder_output_size, self.attention.decoder_output_size))\n",
        "\n",
        "        # From Peter, need to delete\n",
        "        context = torch.zeros(batch_size, self.attention.projection_size).to(DEVICE)\n",
        "\n",
        "\n",
        "        for t in range(timesteps):\n",
        "\n",
        "            #TODO: Generate the embedding for the character at timestep t\n",
        "            char_embed = self.embedding(char)\n",
        "\n",
        "            if self.training and t > 0:\n",
        "                # TODO: We want to decide which embedding to use as input for the decoder during training\n",
        "                # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n",
        "                # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n",
        "                # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n",
        "\n",
        "                # Hmm... shouldn't this code be elsewhere? Not sure where we would get the embedding from the previous timestep.\n",
        "                p = random.random()\n",
        "                if (p < tf_rate):\n",
        "                    char_embed = label_embed[:, t-1, :]\n",
        "\n",
        "                else:\n",
        "                    # Could this be correct?\n",
        "                    char_embed = predictions[t-1]\n",
        "\n",
        "      \n",
        "            # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n",
        "            decoder_input_embedding = torch.cat((char_embed, context), dim=1)\n",
        "            \n",
        "            # Loop over your lstm cells\n",
        "            # Each lstm cell takes in an embedding \n",
        "            for i in range(len(self.lstm_cells)):\n",
        "                # An LSTM Cell returns (h,c) -> h = hidden state, c = cell memory state\n",
        "                # Using 2 LSTM Cells is akin to a 2 layer LSTM looped through t timesteps \n",
        "                # The second LSTM Cell takes in the output hidden state of the first LSTM Cell (from the current timestep) as Input, along with the hidden and cell states of the cell from the previous timestep\n",
        "                hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n",
        "                decoder_input_embedding = hidden_states[i][0]\n",
        "\n",
        "            # The output embedding from the decoder is the hidden state of the last LSTM Cell\n",
        "            decoder_output_embedding = hidden_states[-1][0]\n",
        "\n",
        "            # We compute attention from the output of the last LSTM Cell\n",
        "            if self.attention != None:\n",
        "                context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n",
        "\n",
        "            attention_plot.append(attention_weights[0].detach().cpu())\n",
        "            #attention_plot.append(attention_weights[0].detach())\n",
        "\n",
        "            # TODO: Concatenate the projected query with context for the output embedding\n",
        "            output_embedding     = torch.cat((context, self.attention.query.squeeze(dim=2)), dim=1 )\n",
        "            # Hint: How can you get the projected query from attention\n",
        "            # If you are not using attention, what will you use instead of query?\n",
        "\n",
        "            char_prob            = self.char_prob(output_embedding)\n",
        "            \n",
        "            # Append the character probability distribution to the list of predictions \n",
        "            predictions.append(char_prob)\n",
        "\n",
        "            # TODO: Get the predicted character for the next timestep from the probability distribution \n",
        "            char = char_prob.argmax(1)\n",
        "            # (Hint: Use Greedy Decoding for starters)\n",
        "\n",
        "        # TODO: Stack list of attetion_plots \n",
        "        attention_plot  = torch.stack(attention_plot, dim=1)\n",
        "        # TODO: Stack list of predictions \n",
        "        predictions     = torch.stack(predictions, dim=1)\n",
        "\n",
        "        return predictions, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgncQmVMnCO"
      },
      "source": [
        "## Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWWzurvXM0iv"
      },
      "source": [
        "### LAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "outputs": [],
      "source": [
        "class LAS(torch.nn.Module):\n",
        "    def __init__(self, input_size, encoder_hidden_size, \n",
        "                 vocab_size, embed_size,\n",
        "                 decoder_hidden_size, decoder_output_size,\n",
        "                 projection_size= 128):\n",
        "        \n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder        = Listener(input_size, encoder_hidden_size).to(DEVICE) # TODO: Initialize Encoder\n",
        "        attention_module    = Attention(encoder_hidden_size, decoder_output_size, projection_size).to(DEVICE)# TODO: Initialize Attention\n",
        "        self.decoder        = Speller(embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module = attention_module).to(DEVICE)# TODO: Initialize Decoder, make sure you pass the attention module \n",
        "\n",
        "    def forward(self, x, x_lens, y = None, tf_rate = 1):\n",
        "\n",
        "        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n",
        "        predictions, attention_plot = self.decoder(encoder_outputs, encoder_lens, y, tf_rate)\n",
        "        \n",
        "        return predictions, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMzR6fLht5n"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI2AKhQ6YP6F"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "fBRMVxrz34NS"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alxXeVn640Ww",
        "outputId": "261f2bda-9802-49f9-f6bb-bb57107ac180"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([96, 176, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 334
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "id": "IS-YUHQlYQmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e19c0586-be1a-4e34-df88-b938c5e02244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAS(\n",
            "  (encoder): Listener(\n",
            "    (conv1d): Conv1d(15, 15, kernel_size=(1,), stride=(1,))\n",
            "    (blstm): LSTM(15, 256, batch_first=True, bidirectional=True)\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (1): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Speller(\n",
            "    (embedding): Embedding(43, 256, padding_idx=39)\n",
            "    (lstm_cells): Sequential(\n",
            "      (0): LSTMCell(384, 512)\n",
            "      (1): LSTMCell(512, 128)\n",
            "    )\n",
            "    (char_prob): Linear(in_features=256, out_features=43, bias=True)\n",
            "    (attention): Attention(\n",
            "      (key_projection): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (value_projection): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (softmax): Softmax(dim=1)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "===============================================================================================\n",
            "                                              Kernel Shape   Output Shape  \\\n",
            "Layer                                                                       \n",
            "0_encoder.LSTM_blstm                                     -    [2208, 512]   \n",
            "1_encoder.pBLSTMs.0.LSTM_blstm                           -    [1056, 512]   \n",
            "2_encoder.pBLSTMs.1.LSTM_blstm                           -     [480, 512]   \n",
            "3_encoder.pBLSTMs.2.LSTM_blstm                           -     [192, 512]   \n",
            "4_decoder.Embedding_embedding                    [256, 43]  [96, 23, 256]   \n",
            "5_decoder.attention.Linear_key_projection       [512, 128]   [96, 2, 128]   \n",
            "6_decoder.attention.Linear_value_projection     [512, 128]   [96, 2, 128]   \n",
            "7_decoder.Embedding_embedding                    [256, 43]      [96, 256]   \n",
            "8_decoder.lstm_cells.LSTMCell_0                          -      [96, 512]   \n",
            "9_decoder.lstm_cells.LSTMCell_1                          -      [96, 128]   \n",
            "10_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "11_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "12_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "13_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "14_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "15_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "16_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "17_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "18_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "19_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "20_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "21_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "22_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "23_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "24_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "25_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "26_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "27_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "28_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "29_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "30_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "31_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "32_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "33_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "34_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "35_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "36_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "37_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "38_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "39_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "40_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "41_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "42_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "43_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "44_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "45_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "46_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "47_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "48_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "49_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "50_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "51_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "52_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "53_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "54_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "55_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "56_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "57_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "58_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "59_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "60_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "61_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "62_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "63_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "64_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "65_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "66_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "67_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "68_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "69_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "70_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "71_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "72_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "73_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "74_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "75_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "76_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "77_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "78_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "79_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "80_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "81_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "82_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "83_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "84_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "85_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "86_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "87_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "88_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "89_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "90_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "91_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "92_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "93_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "94_decoder.attention.Linear_query_projection    [128, 128]      [96, 128]   \n",
            "95_decoder.attention.Softmax_softmax                     -        [96, 2]   \n",
            "96_decoder.Linear_char_prob                      [256, 43]       [96, 43]   \n",
            "97_decoder.Embedding_embedding                   [256, 43]      [96, 256]   \n",
            "98_decoder.lstm_cells.LSTMCell_0                         -      [96, 512]   \n",
            "99_decoder.lstm_cells.LSTMCell_1                         -      [96, 128]   \n",
            "100_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "101_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "102_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "103_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "104_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "105_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "106_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "107_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "108_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "109_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "110_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "111_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "112_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "113_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "114_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "115_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "116_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "117_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "118_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "119_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "120_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "121_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "122_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "123_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "124_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "125_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "126_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "127_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "128_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "129_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "130_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "131_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "132_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "133_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "134_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "135_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "136_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "137_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "138_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "139_decoder.Embedding_embedding                  [256, 43]      [96, 256]   \n",
            "140_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
            "141_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
            "142_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
            "143_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
            "144_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
            "\n",
            "                                                  Params  Mult-Adds  \n",
            "Layer                                                                \n",
            "0_encoder.LSTM_blstm                            559.104k   555.008k  \n",
            "1_encoder.pBLSTMs.0.LSTM_blstm                 2.625536M   2.62144M  \n",
            "2_encoder.pBLSTMs.1.LSTM_blstm                 2.625536M   2.62144M  \n",
            "3_encoder.pBLSTMs.2.LSTM_blstm                 2.625536M   2.62144M  \n",
            "4_decoder.Embedding_embedding                    11.008k    11.008k  \n",
            "5_decoder.attention.Linear_key_projection        65.664k    65.536k  \n",
            "6_decoder.attention.Linear_value_projection      65.664k    65.536k  \n",
            "7_decoder.Embedding_embedding                          -    11.008k  \n",
            "8_decoder.lstm_cells.LSTMCell_0                1.839104M  1.835008M  \n",
            "9_decoder.lstm_cells.LSTMCell_1                 328.704k    327.68k  \n",
            "10_decoder.attention.Linear_query_projection     16.512k    16.384k  \n",
            "11_decoder.attention.Softmax_softmax                   -          -  \n",
            "12_decoder.Linear_char_prob                      11.051k    11.008k  \n",
            "13_decoder.Embedding_embedding                         -    11.008k  \n",
            "14_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "15_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "16_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "17_decoder.attention.Softmax_softmax                   -          -  \n",
            "18_decoder.Linear_char_prob                            -    11.008k  \n",
            "19_decoder.Embedding_embedding                         -    11.008k  \n",
            "20_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "21_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "22_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "23_decoder.attention.Softmax_softmax                   -          -  \n",
            "24_decoder.Linear_char_prob                            -    11.008k  \n",
            "25_decoder.Embedding_embedding                         -    11.008k  \n",
            "26_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "27_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "28_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "29_decoder.attention.Softmax_softmax                   -          -  \n",
            "30_decoder.Linear_char_prob                            -    11.008k  \n",
            "31_decoder.Embedding_embedding                         -    11.008k  \n",
            "32_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "33_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "34_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "35_decoder.attention.Softmax_softmax                   -          -  \n",
            "36_decoder.Linear_char_prob                            -    11.008k  \n",
            "37_decoder.Embedding_embedding                         -    11.008k  \n",
            "38_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "39_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "40_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "41_decoder.attention.Softmax_softmax                   -          -  \n",
            "42_decoder.Linear_char_prob                            -    11.008k  \n",
            "43_decoder.Embedding_embedding                         -    11.008k  \n",
            "44_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "45_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "46_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "47_decoder.attention.Softmax_softmax                   -          -  \n",
            "48_decoder.Linear_char_prob                            -    11.008k  \n",
            "49_decoder.Embedding_embedding                         -    11.008k  \n",
            "50_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "51_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "52_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "53_decoder.attention.Softmax_softmax                   -          -  \n",
            "54_decoder.Linear_char_prob                            -    11.008k  \n",
            "55_decoder.Embedding_embedding                         -    11.008k  \n",
            "56_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "57_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "58_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "59_decoder.attention.Softmax_softmax                   -          -  \n",
            "60_decoder.Linear_char_prob                            -    11.008k  \n",
            "61_decoder.Embedding_embedding                         -    11.008k  \n",
            "62_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "63_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "64_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "65_decoder.attention.Softmax_softmax                   -          -  \n",
            "66_decoder.Linear_char_prob                            -    11.008k  \n",
            "67_decoder.Embedding_embedding                         -    11.008k  \n",
            "68_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "69_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "70_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "71_decoder.attention.Softmax_softmax                   -          -  \n",
            "72_decoder.Linear_char_prob                            -    11.008k  \n",
            "73_decoder.Embedding_embedding                         -    11.008k  \n",
            "74_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "75_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "76_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "77_decoder.attention.Softmax_softmax                   -          -  \n",
            "78_decoder.Linear_char_prob                            -    11.008k  \n",
            "79_decoder.Embedding_embedding                         -    11.008k  \n",
            "80_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "81_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "82_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "83_decoder.attention.Softmax_softmax                   -          -  \n",
            "84_decoder.Linear_char_prob                            -    11.008k  \n",
            "85_decoder.Embedding_embedding                         -    11.008k  \n",
            "86_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "87_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "88_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "89_decoder.attention.Softmax_softmax                   -          -  \n",
            "90_decoder.Linear_char_prob                            -    11.008k  \n",
            "91_decoder.Embedding_embedding                         -    11.008k  \n",
            "92_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "93_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "94_decoder.attention.Linear_query_projection           -    16.384k  \n",
            "95_decoder.attention.Softmax_softmax                   -          -  \n",
            "96_decoder.Linear_char_prob                            -    11.008k  \n",
            "97_decoder.Embedding_embedding                         -    11.008k  \n",
            "98_decoder.lstm_cells.LSTMCell_0                       -  1.835008M  \n",
            "99_decoder.lstm_cells.LSTMCell_1                       -    327.68k  \n",
            "100_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "101_decoder.attention.Softmax_softmax                  -          -  \n",
            "102_decoder.Linear_char_prob                           -    11.008k  \n",
            "103_decoder.Embedding_embedding                        -    11.008k  \n",
            "104_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "105_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "106_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "107_decoder.attention.Softmax_softmax                  -          -  \n",
            "108_decoder.Linear_char_prob                           -    11.008k  \n",
            "109_decoder.Embedding_embedding                        -    11.008k  \n",
            "110_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "111_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "112_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "113_decoder.attention.Softmax_softmax                  -          -  \n",
            "114_decoder.Linear_char_prob                           -    11.008k  \n",
            "115_decoder.Embedding_embedding                        -    11.008k  \n",
            "116_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "117_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "118_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "119_decoder.attention.Softmax_softmax                  -          -  \n",
            "120_decoder.Linear_char_prob                           -    11.008k  \n",
            "121_decoder.Embedding_embedding                        -    11.008k  \n",
            "122_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "123_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "124_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "125_decoder.attention.Softmax_softmax                  -          -  \n",
            "126_decoder.Linear_char_prob                           -    11.008k  \n",
            "127_decoder.Embedding_embedding                        -    11.008k  \n",
            "128_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "129_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "130_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "131_decoder.attention.Softmax_softmax                  -          -  \n",
            "132_decoder.Linear_char_prob                           -    11.008k  \n",
            "133_decoder.Embedding_embedding                        -    11.008k  \n",
            "134_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "135_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "136_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "137_decoder.attention.Softmax_softmax                  -          -  \n",
            "138_decoder.Linear_char_prob                           -    11.008k  \n",
            "139_decoder.Embedding_embedding                        -    11.008k  \n",
            "140_decoder.lstm_cells.LSTMCell_0                      -  1.835008M  \n",
            "141_decoder.lstm_cells.LSTMCell_1                      -    327.68k  \n",
            "142_decoder.attention.Linear_query_projection          -    16.384k  \n",
            "143_decoder.attention.Softmax_softmax                  -          -  \n",
            "144_decoder.Linear_char_prob                           -    11.008k  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          10.773419M\n",
            "Trainable params      10.773419M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             59.186432M\n",
            "===============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Kernel Shape   Output Shape  \\\n",
              "Layer                                                                       \n",
              "0_encoder.LSTM_blstm                                     -    [2208, 512]   \n",
              "1_encoder.pBLSTMs.0.LSTM_blstm                           -    [1056, 512]   \n",
              "2_encoder.pBLSTMs.1.LSTM_blstm                           -     [480, 512]   \n",
              "3_encoder.pBLSTMs.2.LSTM_blstm                           -     [192, 512]   \n",
              "4_decoder.Embedding_embedding                    [256, 43]  [96, 23, 256]   \n",
              "...                                                    ...            ...   \n",
              "140_decoder.lstm_cells.LSTMCell_0                        -      [96, 512]   \n",
              "141_decoder.lstm_cells.LSTMCell_1                        -      [96, 128]   \n",
              "142_decoder.attention.Linear_query_projection   [128, 128]      [96, 128]   \n",
              "143_decoder.attention.Softmax_softmax                    -        [96, 2]   \n",
              "144_decoder.Linear_char_prob                     [256, 43]       [96, 43]   \n",
              "\n",
              "                                                  Params  Mult-Adds  \n",
              "Layer                                                                \n",
              "0_encoder.LSTM_blstm                            559104.0   555008.0  \n",
              "1_encoder.pBLSTMs.0.LSTM_blstm                 2625536.0  2621440.0  \n",
              "2_encoder.pBLSTMs.1.LSTM_blstm                 2625536.0  2621440.0  \n",
              "3_encoder.pBLSTMs.2.LSTM_blstm                 2625536.0  2621440.0  \n",
              "4_decoder.Embedding_embedding                    11008.0    11008.0  \n",
              "...                                                  ...        ...  \n",
              "140_decoder.lstm_cells.LSTMCell_0                    NaN  1835008.0  \n",
              "141_decoder.lstm_cells.LSTMCell_1                    NaN   327680.0  \n",
              "142_decoder.attention.Linear_query_projection        NaN    16384.0  \n",
              "143_decoder.attention.Softmax_softmax                NaN        NaN  \n",
              "144_decoder.Linear_char_prob                         NaN    11008.0  \n",
              "\n",
              "[145 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b875088-30cd-4af6-b972-57f59a981d8f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_encoder.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[2208, 512]</td>\n",
              "      <td>559104.0</td>\n",
              "      <td>555008.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[1056, 512]</td>\n",
              "      <td>2625536.0</td>\n",
              "      <td>2621440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_encoder.pBLSTMs.1.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[480, 512]</td>\n",
              "      <td>2625536.0</td>\n",
              "      <td>2621440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_encoder.pBLSTMs.2.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[192, 512]</td>\n",
              "      <td>2625536.0</td>\n",
              "      <td>2621440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_decoder.Embedding_embedding</th>\n",
              "      <td>[256, 43]</td>\n",
              "      <td>[96, 23, 256]</td>\n",
              "      <td>11008.0</td>\n",
              "      <td>11008.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140_decoder.lstm_cells.LSTMCell_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[96, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1835008.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141_decoder.lstm_cells.LSTMCell_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[96, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>327680.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142_decoder.attention.Linear_query_projection</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>[96, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16384.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143_decoder.attention.Softmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[96, 2]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144_decoder.Linear_char_prob</th>\n",
              "      <td>[256, 43]</td>\n",
              "      <td>[96, 43]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11008.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b875088-30cd-4af6-b972-57f59a981d8f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b875088-30cd-4af6-b972-57f59a981d8f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b875088-30cd-4af6-b972-57f59a981d8f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ],
      "source": [
        "# Baseline LAS has the following configuration:\n",
        "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
        "# Decoder Embedding Layer Dimension of 256\n",
        "# Decoder Hidden Dimension of 512 \n",
        "# Decoder Output Dimension of 128\n",
        "# Attention Projection Size of 128\n",
        "# Feel Free to Experiment with this \n",
        "\n",
        "# class LAS(torch.nn.Module):\n",
        "#     def __init__(self, input_size, encoder_hidden_size, \n",
        "#                  vocab_size, embed_size,\n",
        "#                  decoder_hidden_size, decoder_output_size,\n",
        "#                  projection_size= 128)\n",
        "\n",
        "model = LAS(\n",
        "    input_size = 15,\n",
        "    encoder_hidden_size = 256,\n",
        "    vocab_size =  len(VOCAB),\n",
        "    embed_size = 256,\n",
        "    decoder_hidden_size = 512,\n",
        "    decoder_output_size = 128,\n",
        "    projection_size = 128\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "x= example_batch[0].to(DEVICE)\n",
        "x_lens= example_batch[3]\n",
        "y= example_batch[1].to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "summary(model, \n",
        "        x=x, \n",
        "        x_lens=x_lens, \n",
        "        y= example_batch[1].to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDmcYul-YSdC"
      },
      "source": [
        "## Optimizer, Scheduler, Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "id": "_HwmgDSvbtmd"
      },
      "outputs": [],
      "source": [
        "optimizer   = torch.optim.Adam(model.parameters(), lr= config['lr'], amsgrad= True, weight_decay= 5e-6)\n",
        "criterion   = torch.nn.CrossEntropyLoss(reduction='none') # Why are we using reduction = 'none' ? \n",
        "scaler      = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Optional: Create a custom class for a Teacher Force Schedule "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baHCja89YV-m"
      },
      "source": [
        "# Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "id": "GDYZnnLbqJ8J"
      },
      "outputs": [],
      "source": [
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if vocab[int(i)] == vocab[SOS_TOKEN]: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif vocab[int(i)] == vocab[EOS_TOKEN]: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[i])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size): \n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        \n",
        "        #dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above abd uncomment below for toy dataset \n",
        "        dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "\n",
        "    if print_example: \n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"Ground Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "        \n",
        "    dist/=batch_size\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjfF88iZ4Nc"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "id": "wIXzhQclhs98"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate=1):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "    \n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        #with torch.cuda.amp.autocast():\n",
        "\n",
        "        predictions, attention_plot = model(x, lx, y= y, tf_rate= teacher_forcing_rate)\n",
        "\n",
        "        # Predictions are of Shape (batch_size, timesteps, vocab_size). \n",
        "        # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "        # So in total, you have batch_size*timesteps amount of characters.\n",
        "        # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "        # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "        # TODO: Cross Entropy Loss\n",
        "\n",
        "        x = torch.permute(predictions, (0, 2, 1))\n",
        "        loss        =  criterion(x, y) # Question: what is the shape of the loss?\n",
        "\n",
        "        # TODO: Create a boolean mask using the lengths of your transcript that remove the influence of padding indices (in transcripts) in the loss \n",
        "        mask        = (torch.arange(23).to(DEVICE)) <= (ly.unsqueeze(1).to(DEVICE))\n",
        "        # Product between the mask and the loss, divided by the mask's sum. Hint: You may want to reshape the mask too\n",
        "        masked_loss =  torch.sum((mask * loss) / torch.sum(mask))\n",
        "        # Perplexity is defined the exponential of the loss\n",
        "        perplexity  = torch.exp(masked_loss) \n",
        "\n",
        "        running_loss        += masked_loss.item()\n",
        "        running_perplexity  += perplexity.item()\n",
        "        \n",
        "        # Backward on the masked loss\n",
        "        #scaler.scale(masked_loss).backward()\n",
        "        #scaler.scale(loss).backward()\n",
        "        masked_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "        \n",
        "        #scaler.step(optimizer)\n",
        "        #scaler.update()\n",
        "        \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "collapsed": true,
        "id": "jzpCjd9R5VYV"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        greedy_predictions   =  predictions.argmax(dim=-1)# TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        # Calculate Levenshtein Distance\n",
        "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist#, running_loss, running_perplexity, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9uRTbbPkeL-"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "qVVo8bAab_5N"
      },
      "outputs": [],
      "source": [
        "# Login to Wandb\n",
        "# Initialize your Wandb Run Here\n",
        "# Optional: Save your model architecture in a txt file, and save the file to Wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rJQUdPkCUq"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "id": "s12S_bPMcguA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804,
          "referenced_widgets": [
            "b180f674204d40dfa304988c63ef6b72",
            "d9dac433640b45c8ad1d623b586e0202",
            "b368700effa845f4a3067629f02e1a0a",
            "02b19afcfe2d45b2b83fab22206694b4",
            "bb883f1cee6743d2a5221f7bc7ad178d",
            "07f09d7562e7442da4b7009fa26a1a8d",
            "4574c528e156459eae54faf97f28d226",
            "557aa8192be440ecb8630154ebd07763",
            "e0a3ba00b21e430794e99adbf7f97026",
            "32974c25ea784aa69c512425249e135f",
            "fe941618e2564dd1a37d948fa88ce9f2",
            "f5978c1d38fe445c82ac14ecdf09f33c",
            "723204f14a3c4a37bd28261e2ed0fdc7",
            "b2f1d4e5650b4f7b91dc3e3648368375",
            "da01fba3f35b4272acbfb2276bf8e3ad",
            "e6965418b6e2402fa1cafbd6025bb373",
            "7d9d6673a8704e1a918716d04f425ec4",
            "0a4c7e68350344b69ca362ebcf44ce81",
            "8fb2934baf0c4b5d9fc629d97dc56fe3",
            "ed8984566c37422a859b6e776a85a066",
            "8ffc8b0e3d58442d88a50ede64085660",
            "b749fd3903c1484ea10a92ee0d1b302e",
            "a9229fec83314b8185bf899aeb9c89ea",
            "c0ad5f200c5e4037b18ee090fa45c641",
            "df186fa0cd10431eafb57755d00fdf50",
            "216cfb405e1643eea71182591a792a76",
            "960ba42d777f45cea032a27a2b474208",
            "03ac1dab9194425e90480be31e77bd3c",
            "31a60a2c46f9422c8202d06ee6677034",
            "5d53efa7c0aa451ba68baf2c0b205f43",
            "e1ddbc6f8533418288e4e774e4580ebb",
            "32a1db83d2384545a82ae4db3fcaf83e",
            "c8d38ddd32da44d690638d113833c656"
          ]
        },
        "outputId": "06d3a066-0f3b-4c77-ad68-eb455bc560b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1/30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Train:   0%|          | 0/167 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b180f674204d40dfa304988c63ef6b72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Val:   0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5978c1d38fe445c82ac14ecdf09f33c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss:0.09682909368041033 Train Perplexity: 1.102406638111183\n",
            "\n",
            "Val Dist: 7.814031862745099\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2/30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Train:   0%|          | 0/167 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9229fec83314b8185bf899aeb9c89ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-343-fccadab5a52c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Call train and validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-338-7ba066986aeb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, teacher_forcing_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#with torch.cuda.amp.autocast():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_rate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mteacher_forcing_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Predictions are of Shape (batch_size, timesteps, vocab_size).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-332-51a43df5f17f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_lens, y, tf_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# from Listener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-331-f638abcabe3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_outputs, encoder_lens, y, tf_rate)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# TODO: Concatenate the projected query with context for the output embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0moutput_embedding\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;31m# Hint: How can you get the projected query from attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# If you are not using attention, what will you use instead of query?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_lev_dist = float(\"inf\")\n",
        "tf_rate = 1.0\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "    \n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # Call train and validate \n",
        "    loss, perplexity, attention_plot = train(model, train_loader, criterion, optimizer, teacher_forcing_rate=1)\n",
        "    valid_dist = validate(model, val_loader)\n",
        "\n",
        "    # Print your metrics\n",
        "    print(f\"\\nTrain Loss:{loss} Train Perplexity: {perplexity}\")\n",
        "    print(f\"\\nVal Dist: {valid_dist}\")\n",
        "\n",
        "    # Plot Attention \n",
        "    plot_attention(attention_plot.squeeze(dim=0))\n",
        "\n",
        "    # Log metrics to Wandb\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        # Save your model checkpoint here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3iickk_kJNB"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7HihzA4ViiR"
      },
      "outputs": [],
      "source": [
        "# Optional: Load your best model Checkpoint here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTBD49c_kKs3"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a testing function similar to validation \n",
        "# TODO: Create a file with all predictions \n",
        "# TODO: Submit to Kaggle"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Vlev_Tvq_bRz",
        "lr4xGzRU-KZz",
        "OALQCI0EDCwh",
        "X7J4sY1OW9Pr",
        "YWRjucnUdbQ1",
        "i5ioyn6ldQB9",
        "gQenneVsDLnX",
        "lCbwz0LZMWwe",
        "XoI0zEoIMX5I",
        "_rchbyjlMeB2",
        "f6k9R7jKMRcZ",
        "TDmcYul-YSdC",
        "baHCja89YV-m",
        "99rJQUdPkCUq"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b180f674204d40dfa304988c63ef6b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9dac433640b45c8ad1d623b586e0202",
              "IPY_MODEL_b368700effa845f4a3067629f02e1a0a",
              "IPY_MODEL_02b19afcfe2d45b2b83fab22206694b4"
            ],
            "layout": "IPY_MODEL_bb883f1cee6743d2a5221f7bc7ad178d"
          }
        },
        "d9dac433640b45c8ad1d623b586e0202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f09d7562e7442da4b7009fa26a1a8d",
            "placeholder": "​",
            "style": "IPY_MODEL_4574c528e156459eae54faf97f28d226",
            "value": "Train: 100%"
          }
        },
        "b368700effa845f4a3067629f02e1a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_557aa8192be440ecb8630154ebd07763",
            "max": 167,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0a3ba00b21e430794e99adbf7f97026",
            "value": 167
          }
        },
        "02b19afcfe2d45b2b83fab22206694b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32974c25ea784aa69c512425249e135f",
            "placeholder": "​",
            "style": "IPY_MODEL_fe941618e2564dd1a37d948fa88ce9f2",
            "value": " 167/167 [00:31&lt;00:00,  5.30it/s, loss=0.0968, lr=0.0010, perplexity=1.1024, tf_rate=1.00]"
          }
        },
        "bb883f1cee6743d2a5221f7bc7ad178d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "07f09d7562e7442da4b7009fa26a1a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4574c528e156459eae54faf97f28d226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "557aa8192be440ecb8630154ebd07763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0a3ba00b21e430794e99adbf7f97026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32974c25ea784aa69c512425249e135f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe941618e2564dd1a37d948fa88ce9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5978c1d38fe445c82ac14ecdf09f33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_723204f14a3c4a37bd28261e2ed0fdc7",
              "IPY_MODEL_b2f1d4e5650b4f7b91dc3e3648368375",
              "IPY_MODEL_da01fba3f35b4272acbfb2276bf8e3ad"
            ],
            "layout": "IPY_MODEL_e6965418b6e2402fa1cafbd6025bb373"
          }
        },
        "723204f14a3c4a37bd28261e2ed0fdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d9d6673a8704e1a918716d04f425ec4",
            "placeholder": "​",
            "style": "IPY_MODEL_0a4c7e68350344b69ca362ebcf44ce81",
            "value": "Val: 100%"
          }
        },
        "b2f1d4e5650b4f7b91dc3e3648368375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb2934baf0c4b5d9fc629d97dc56fe3",
            "max": 17,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed8984566c37422a859b6e776a85a066",
            "value": 17
          }
        },
        "da01fba3f35b4272acbfb2276bf8e3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ffc8b0e3d58442d88a50ede64085660",
            "placeholder": "​",
            "style": "IPY_MODEL_b749fd3903c1484ea10a92ee0d1b302e",
            "value": " 17/17 [00:10&lt;00:00,  1.66it/s, dist=7.8140]"
          }
        },
        "e6965418b6e2402fa1cafbd6025bb373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "7d9d6673a8704e1a918716d04f425ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4c7e68350344b69ca362ebcf44ce81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fb2934baf0c4b5d9fc629d97dc56fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed8984566c37422a859b6e776a85a066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ffc8b0e3d58442d88a50ede64085660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b749fd3903c1484ea10a92ee0d1b302e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9229fec83314b8185bf899aeb9c89ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0ad5f200c5e4037b18ee090fa45c641",
              "IPY_MODEL_df186fa0cd10431eafb57755d00fdf50",
              "IPY_MODEL_216cfb405e1643eea71182591a792a76"
            ],
            "layout": "IPY_MODEL_960ba42d777f45cea032a27a2b474208"
          }
        },
        "c0ad5f200c5e4037b18ee090fa45c641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ac1dab9194425e90480be31e77bd3c",
            "placeholder": "​",
            "style": "IPY_MODEL_31a60a2c46f9422c8202d06ee6677034",
            "value": "Train:  32%"
          }
        },
        "df186fa0cd10431eafb57755d00fdf50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d53efa7c0aa451ba68baf2c0b205f43",
            "max": 167,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1ddbc6f8533418288e4e774e4580ebb",
            "value": 54
          }
        },
        "216cfb405e1643eea71182591a792a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a1db83d2384545a82ae4db3fcaf83e",
            "placeholder": "​",
            "style": "IPY_MODEL_c8d38ddd32da44d690638d113833c656",
            "value": " 54/167 [00:09&lt;00:19,  5.92it/s, loss=0.0817, lr=0.0010, perplexity=1.0864, tf_rate=1.00]"
          }
        },
        "960ba42d777f45cea032a27a2b474208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "03ac1dab9194425e90480be31e77bd3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a60a2c46f9422c8202d06ee6677034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d53efa7c0aa451ba68baf2c0b205f43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1ddbc6f8533418288e4e774e4580ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32a1db83d2384545a82ae4db3fcaf83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d38ddd32da44d690638d113833c656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}